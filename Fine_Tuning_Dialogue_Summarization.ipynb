{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOJi7paPVDH0M8DwjuS1efD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sairika/Dialogue-Summarization/blob/main/Fine_Tuning_Dialogue_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Required Packages\n",
        "\n",
        "Before proceeding with the notebook, we need to install a set of essential libraries for training, evaluating, and visualizing transformer-based models. These libraries include tools for natural language processing, evaluation metrics, model fine-tuning, and data visualization.\n",
        "\n"
      ],
      "metadata": {
        "id": "c9J3LJOHsDxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L75wR-psmRRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b611a1a1-e8f0-4a12-bbcc-cbb106c7d631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.1/211.5 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m^C\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q evaluate rouge_score transformers datasets peft accelerate matplotlib seaborn plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Setup\n",
        "\n",
        "In this section, we import all the necessary libraries and perform initial setup tasks such as setting random seeds and checking for GPU availability.\n",
        "\n",
        "### General Purpose Libraries\n",
        "- `torch`, `numpy`, `pandas`: Core libraries for tensor operations, numerical computation, and data manipulation.\n",
        "- `matplotlib.pyplot`, `seaborn`: Visualization tools for statistical and static plots.\n",
        "- `plotly.graph_objects`, `plotly.express`, `plotly.subplots`: Interactive visualization tools for creating dynamic and multi-panel charts.\n",
        "- `warnings`: Used to suppress unnecessary warnings.\n",
        "- `time`, `gc`: Utility modules for timing operations and managing memory.\n",
        "\n",
        "### Hugging Face Ecosystem\n",
        "- `datasets`: To load and process benchmark NLP datasets.\n",
        "- `transformers`: Includes pre-trained transformer models and tools for tokenization, training, and inference.\n",
        "- `peft`: For efficient fine-tuning using methods like LoRA (Low-Rank Adaptation).\n",
        "- `evaluate`: Provides access to standard NLP evaluation metrics.\n",
        "\n",
        "###  Environment Setup\n",
        "- Random seeds are set for reproducibility of experiments.\n",
        "- GPU availability is checked and displayed for transparency.\n",
        "\n"
      ],
      "metadata": {
        "id": "UUxdALFMsIzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import evaluate\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠ WARNING: No GPU detected! Training will be slow.\")"
      ],
      "metadata": {
        "id": "Xi3BkXXUm4XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Loading and Exploring the Dataset\n",
        "\n",
        "In this step, we load the **DialogSum** dataset, a benchmark dataset for dialogue summarization tasks. It consists of dialogue transcripts along with human-written summaries.\n",
        "\n",
        "We use Hugging Face's `datasets` library to directly load the dataset in a structured format with separate splits for training, validation, and testing.\n"
      ],
      "metadata": {
        "id": "Cvpx-Q4Psdv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DialogSum dataset\n",
        "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"📈 Training examples: {len(dataset['train']):,}\")\n",
        "print(f\"📈 Validation examples: {len(dataset['validation']):,}\")\n",
        "print(f\"📈 Test examples: {len(dataset['test']):,}\")"
      ],
      "metadata": {
        "id": "xAvlnxMkm9W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examining Sample Data\n",
        "\n",
        "Before preprocessing and fine-tuning, it's essential to understand the structure and content of the data. Below, we display a sample example from the **DialogSum** training set.\n"
      ],
      "metadata": {
        "id": "NFJp7jnsszqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's examine some examples\n",
        "print(f\"\\n📝 Sample Dialogue and Summary:\")\n",
        "print(\"-\" * 50)\n",
        "sample = dataset['train'][0]\n",
        "print(f\"Dialogue:\\n{sample['dialogue'][:300]}...\")\n",
        "print(f\"\\nSummary:\\n{sample['summary']}\")"
      ],
      "metadata": {
        "id": "GubPv1qmnCR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show statistics about the dataset\n",
        "dialogue_lengths = [len(dialogue.split()) for dialogue in dataset['train']['dialogue']]\n",
        "summary_lengths = [len(summary.split()) for summary in dataset['train']['summary']]\n",
        "\n",
        "print(f\"\\n📊 Dataset Statistics:\")\n",
        "print(f\"Average dialogue length: {np.mean(dialogue_lengths):.1f} words\")\n",
        "print(f\"Average summary length: {np.mean(summary_lengths):.1f} words\")\n",
        "print(f\"Max dialogue length: {max(dialogue_lengths)} words\")\n",
        "print(f\"Max summary length: {max(summary_lengths)} words\")"
      ],
      "metadata": {
        "id": "-MD1b7vTnCeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize length distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax1.hist(dialogue_lengths[:1000], bins=50, alpha=0.7, color='skyblue')\n",
        "ax1.set_title('Distribution of Dialogue Lengths')\n",
        "ax1.set_xlabel('Number of Words')\n",
        "ax1.set_ylabel('Frequency')\n",
        "\n",
        "ax2.hist(summary_lengths[:1000], bins=30, alpha=0.7, color='lightcoral')\n",
        "ax2.set_title('Distribution of Summary Lengths')\n",
        "ax2.set_xlabel('Number of Words')\n",
        "ax2.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0EfOmYeknCtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Defining Models for Comparison\n",
        "\n",
        "In this step, we define a selection of pre-trained transformer models to compare for the **dialogue summarization** task. Each model varies in size, architecture, and training objective.\n",
        "\n",
        "We will evaluate the models based on summarization quality, speed, and resource usage.\n",
        "\n",
        "### Selected Models:\n",
        "\n",
        "| Model Name         | Parameters | Description |\n",
        "|--------------------|------------|-------------|\n",
        "| **FLAN-T5-Base**   | 248M       | Instruction-tuned T5 model, performs well at following prompts. |\n",
        "| **DistilBART-CNN** | 306M       | A distilled version of BART, trained on CNN/Daily Mail for summarization. Faster and lighter than full BART. |\n",
        "| **T5-Small**       | 60M        | A compact version of T5, offering faster inference with some performance trade-offs. |\n"
      ],
      "metadata": {
        "id": "Lwdc2QIrs-Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🤖 STEP 2: DEFINING MODELS FOR COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define the models we want to compare\n",
        "models_info = {\n",
        "    'FLAN-T5-Base': {\n",
        "        'model_name': 'google/flan-t5-base',\n",
        "        'description': 'Instruction-tuned T5 model, good at following prompts',\n",
        "        'parameters': '248M'\n",
        "    },\n",
        "    'DistilBART-CNN': {\n",
        "        'model_name': 'sshleifer/distilbart-cnn-12-6',\n",
        "        'description': 'Distilled BART model trained on CNN/Daily Mail',\n",
        "        'parameters': '306M'\n",
        "    },\n",
        "    'T5-Small': {\n",
        "        'model_name': 't5-small',\n",
        "        'description': 'Smaller version of T5, faster but potentially less accurate',\n",
        "        'parameters': '60M'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Models selected for comparison:\")\n",
        "for name, info in models_info.items():\n",
        "    print(f\"• {name}: {info['description']} ({info['parameters']} params)\")"
      ],
      "metadata": {
        "id": "AjVo_lCNnC5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Function: `create_prompts`\n",
        "\n",
        "Different models require different input formats to perform optimally. In this step, we define a utility function to generate appropriate prompts based on the model type.\n",
        "\n",
        "This function generates prompts that guide the model during training or inference. It supports both **T5-style** and **BART-style** models.\n",
        "\n",
        "### Prompt Design:\n",
        "- **T5-style models** (e.g., `flan-t5`, `t5-small`) are instruction-tuned. They benefit from clear, task-specific prompts.\n",
        "- **BART-style models** (e.g., `DistilBART-CNN`) do not require explicit task instructions and perform well with direct input.\n"
      ],
      "metadata": {
        "id": "nqiCdpcrtE0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompts(dialogues, model_type):\n",
        "    \"\"\"Create appropriate prompts based on model type\"\"\"\n",
        "    if model_type in ['flan-t5', 't5']:\n",
        "        # T5-style models work better with explicit instructions\n",
        "        prompts = [f\"Summarize the following conversation.\\n\\n{dialogue}\\n\\nSummary: \"\n",
        "                  for dialogue in dialogues]\n",
        "    else:\n",
        "        # BART-style models can work with direct input\n",
        "        prompts = dialogues\n",
        "    return prompts"
      ],
      "metadata": {
        "id": "R2VuOnRdnDGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `tokenize_data`\n",
        "\n",
        "Before training transformer models, we need to convert the raw text data (dialogues and summaries) into token IDs that the model can process. This is done using a tokenizer specific to each pre-trained model.\n",
        "\n",
        "This function tokenizes the entire dataset—both inputs and targets—based on the selected model type and tokenizer.\n",
        "\n",
        "### Key Features:\n",
        "- **Prompt Creation**: Uses the `create_prompts()` function to generate model-specific input prompts.\n",
        "- **Input Tokenization**: Truncates dialogues to a maximum length (default: 512 tokens).\n",
        "- **Target Tokenization**: Summaries are also tokenized with a shorter max length (default: 128 tokens).\n",
        "- **Label Assignment**: The tokenized summaries are stored in the `labels` field as required for Seq2Seq models."
      ],
      "metadata": {
        "id": "hC3uPgE7tXXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(dataset, tokenizer, model_type, max_input_len=512, max_target_len=128):\n",
        "    \"\"\"Tokenize the dataset for a specific model\"\"\"\n",
        "    def tokenize_function(examples):\n",
        "        # Create prompts\n",
        "        prompts = create_prompts(examples[\"dialogue\"], model_type)\n",
        "\n",
        "        # Tokenize inputs\n",
        "        model_inputs = tokenizer(\n",
        "            prompts,\n",
        "            max_length=max_input_len,\n",
        "            truncation=True,\n",
        "            padding=False\n",
        "        )\n",
        "\n",
        "        # Tokenize targets\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary\"],\n",
        "            max_length=max_target_len,\n",
        "            truncation=True,\n",
        "            padding=False\n",
        "        )\n",
        "\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "    return tokenized_dataset"
      ],
      "metadata": {
        "id": "DFcl-TqDnamD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `load_model_and_tokenizer`\n",
        "\n",
        "This function is responsible for loading the **pre-trained model** and its corresponding **tokenizer** from the Hugging Face model hub. It ensures the model is ready for fine-tuning or inference.\n",
        "\n",
        "### Key Features:\n",
        "- **AutoTokenizer**: Automatically selects the right tokenizer class for the given model.\n",
        "- **Pad Token Handling**: If the tokenizer does not have a defined `pad_token`, it falls back to using the `eos_token` to prevent training issues.\n",
        "- **AutoModelForSeq2SeqLM**: Loads a sequence-to-sequence model suitable for summarization tasks.\n",
        "- **Optimized Loading**:\n",
        "  - Loads weights in `bfloat16` for reduced memory usage (where supported).\n",
        "  - Uses `device_map=\"auto\"` to automatically place model parts on available hardware (CPU/GPU).\n"
      ],
      "metadata": {
        "id": "-ehpB56YtmYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"Load model and tokenizer\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    print(f\"✅ {model_name} loaded successfully\")\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "ZpTy-2JVnmzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `generate_summaries`\n",
        "\n",
        "This function generates abstractive summaries for a batch of dialogues using the fine-tuned or pre-trained sequence-to-sequence model.\n",
        "\n",
        "### How It Works:\n",
        "- Processes dialogues in **batches** for efficient inference.\n",
        "- Creates model-specific prompts using the `create_prompts()` function.\n",
        "- Tokenizes the prompts and sends them to the model's device (CPU/GPU).\n",
        "- Uses beam search (`num_beams=4`) with length penalty and early stopping to produce higher-quality summaries.\n",
        "- Decodes generated token IDs back into human-readable text.\n",
        "- Periodically clears CUDA cache during inference to manage GPU memory efficiently.\n"
      ],
      "metadata": {
        "id": "4VEskqIgtyrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summaries(model, tokenizer, dialogues, model_type, batch_size=8):\n",
        "    \"\"\"Generate summaries for given dialogues\"\"\"\n",
        "    summaries = []\n",
        "    model.eval()\n",
        "\n",
        "    for i in range(0, len(dialogues), batch_size):\n",
        "        batch_dialogues = dialogues[i:i+batch_size]\n",
        "        prompts = create_prompts(batch_dialogues, model_type)\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                num_beams=4,\n",
        "                length_penalty=0.6,\n",
        "                early_stopping=True,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        batch_summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        summaries.extend(batch_summaries)\n",
        "\n",
        "        # Clear cache\n",
        "        if i % (batch_size * 5) == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "E1WrxsKHnm8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Function: `evaluate_summaries`\n",
        "\n",
        "To assess the quality of the generated summaries, we use the **ROUGE** metric, a widely used evaluation metric for summarization tasks that compares overlap of n-grams, sequences, and word pairs between the generated text and reference summaries.\n",
        "\n",
        "### Details:\n",
        "- Utilizes the Hugging Face `evaluate` library's built-in `rouge` metric.\n",
        "- Computes ROUGE scores with stemming enabled for better matching.\n",
        "- Returns aggregated ROUGE scores including ROUGE-1, ROUGE-2, and ROUGE-L.\n"
      ],
      "metadata": {
        "id": "0qrZkHkSt9ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_summaries(generated, references):\n",
        "    \"\"\"Evaluate generated summaries using ROUGE scores\"\"\"\n",
        "    rouge = evaluate.load('rouge')\n",
        "    results = rouge.compute(\n",
        "        predictions=generated,\n",
        "        references=references,\n",
        "        use_aggregator=True,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    return results"
      ],
      "metadata": {
        "id": "aP6qxK8mnnFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `show_sample_results`\n",
        "\n",
        "Quantitative metrics like ROUGE provide valuable insights, but manual inspection of generated summaries is essential to understand model behavior qualitatively.\n",
        "\n",
        "This function prints a few examples from the dataset, showing the original dialogue, the human-written reference summary, and the model-generated summary side-by-side for easy comparison."
      ],
      "metadata": {
        "id": "YWX7Oh_NuGUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_results(dialogues, references, generated, model_name, num_samples=3):\n",
        "    \"\"\"Display sample results for manual inspection\"\"\"\n",
        "    print(f\"\\n📝 SAMPLE RESULTS FOR {model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i in range(min(num_samples, len(dialogues))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Dialogue: {dialogues[i][:200]}...\")\n",
        "        print(f\"Reference: {references[i]}\")\n",
        "        print(f\"Generated: {generated[i]}\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "8pC-0QPQnxim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing a Sample for Zero-Shot Evaluation\n",
        "\n",
        "To evaluate the models' performance without any fine-tuning (zero-shot setting), we select a manageable subset from the test split. This allows us to quickly generate and assess summaries while conserving computational resources.\n",
        "\n",
        "### Sampling Details:\n",
        "- Selected the first **1000** examples from the test dataset.\n",
        "- Extracted the dialogues as model inputs.\n",
        "- Extracted the corresponding human-written summaries as references for evaluation.\n"
      ],
      "metadata": {
        "id": "mPIYECtAuM1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔍 STEP 3: ZERO-SHOT EVALUATION (Before Fine-tuning)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Take a small sample for zero-shot evaluation\n",
        "test_sample = dataset['test'].select(range(1000))\n",
        "test_dialogues = test_sample['dialogue']\n",
        "test_references = test_sample['summary']"
      ],
      "metadata": {
        "id": "RdJyXICBny4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Zero-Shot Evaluation of Models\n",
        "\n",
        "In this critical step, we evaluate all selected models on the test subset **without any fine-tuning** (zero-shot). This helps establish a performance baseline for each model on the dialogue summarization task.\n",
        "\n",
        "### Evaluation Workflow:\n",
        "1. **Model Loading**: Each model and its tokenizer are loaded.\n",
        "2. **Summary Generation**: Summaries are generated for the test dialogues using the `generate_summaries()` function.\n",
        "3. **Performance Timing**: The time taken for inference is recorded.\n",
        "4. **Evaluation**: Generated summaries are evaluated against reference summaries using ROUGE metrics.\n",
        "5. **Results Storage**: ROUGE scores, inference times, and generated summaries are stored for later analysis.\n",
        "6. **Sample Inspection**: A few sample results are printed for qualitative assessment.\n",
        "7. **Memory Management**: Model and tokenizer objects are deleted and GPU memory cleared after each iteration to avoid memory overflow."
      ],
      "metadata": {
        "id": "-e1afgAnuiZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_results = {}\n",
        "\n",
        "for model_display_name, model_info in models_info.items():\n",
        "    print(f\"\\n--- Evaluating {model_display_name} (Zero-shot) ---\")\n",
        "\n",
        "    model_name = model_info['model_name']\n",
        "    model_type = 'flan-t5' if 'flan-t5' in model_name else 't5' if 't5' in model_name else 'bart'\n",
        "\n",
        "    # Load model\n",
        "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "    # Generate summaries\n",
        "    start_time = time.time()\n",
        "    generated_summaries = generate_summaries(\n",
        "        model, tokenizer, test_dialogues, model_type\n",
        "    )\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate\n",
        "    rouge_scores = evaluate_summaries(generated_summaries, test_references)\n",
        "\n",
        "    # Store results\n",
        "    zero_shot_results[model_display_name] = {\n",
        "        'rouge_scores': rouge_scores,\n",
        "        'inference_time': inference_time,\n",
        "        'generated_summaries': generated_summaries\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "    print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "    print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "    print(f\"Inference time: {inference_time:.2f}s\")\n",
        "\n",
        "    # Show sample results\n",
        "    show_sample_results(test_dialogues, test_references, generated_summaries,\n",
        "                       model_display_name, num_samples=2)\n",
        "\n",
        "    # Clean up\n",
        "    del model, tokenizer\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "5ZVhc-qhn695"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating all models, we compile their performance metrics into a concise **DataFrame** for easy comparison.\n",
        "\n",
        "### What’s Included:\n",
        "- **Model Name**\n",
        "- **ROUGE Scores**: ROUGE-1, ROUGE-2, and ROUGE-L to assess summary quality.\n",
        "- **Inference Time**: Time taken to generate summaries on the test subset."
      ],
      "metadata": {
        "id": "hkOCda1DumJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison dataframe\n",
        "zero_shot_df = pd.DataFrame([\n",
        "    {\n",
        "        'Model': name,\n",
        "        'ROUGE-1': results['rouge_scores']['rouge1'],\n",
        "        'ROUGE-2': results['rouge_scores']['rouge2'],\n",
        "        'ROUGE-L': results['rouge_scores']['rougeL'],\n",
        "        'Inference Time (s)': results['inference_time']\n",
        "    }\n",
        "    for name, results in zero_shot_results.items()\n",
        "])\n",
        "\n",
        "print(\"Zero-shot Performance Comparison:\")\n",
        "print(zero_shot_df.to_string(index=False, float_format='%.4f'))"
      ],
      "metadata": {
        "id": "i0R9QCydoAG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot zero-shot results\n",
        "fig = make_subplots(\n",
        "    rows=1, cols=2,\n",
        "    subplot_titles=('ROUGE Scores', 'Inference Time'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# ROUGE scores\n",
        "models = zero_shot_df['Model']\n",
        "fig.add_trace(\n",
        "    go.Bar(x=models, y=zero_shot_df['ROUGE-1'], name='ROUGE-1', marker_color='lightblue'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Bar(x=models, y=zero_shot_df['ROUGE-2'], name='ROUGE-2', marker_color='lightgreen'),\n",
        "    row=1, col=1\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Bar(x=models, y=zero_shot_df['ROUGE-L'], name='ROUGE-L', marker_color='lightcoral'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Inference time\n",
        "fig.add_trace(\n",
        "    go.Bar(x=models, y=zero_shot_df['Inference Time (s)'], name='Time', marker_color='orange'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(title_text=\"Zero-shot Model Comparison\", height=500)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lUXOgyiMoKNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Fine-Tuning Models with LoRA (Low-Rank Adaptation)\n",
        "\n",
        "In this step, we prepare the configuration for fine-tuning our selected models using **LoRA**, a parameter-efficient fine-tuning method that updates a small subset of model weights, enabling faster training with less memory usage.\n",
        "\n",
        "### Training Configuration:\n",
        "| Parameter          | Value        | Description                               |\n",
        "|--------------------|--------------|-------------------------------------------|\n",
        "| `batch_size`       | 8            | Number of samples processed per batch    |\n",
        "| `learning_rate`    | 3e-4         | Optimizer learning rate                    |\n",
        "| `num_epochs`       | 2            | Number of passes over the training data (reduced for demo) |\n",
        "| `max_input_length` | 512          | Maximum token length for input dialogues  |\n",
        "| `max_target_length`| 128          | Maximum token length for target summaries |\n",
        "| `lora_r`           | 16           | LoRA rank (dimension of the low-rank matrices) |\n",
        "| `lora_alpha`       | 32           | LoRA scaling factor                        |\n",
        "| `lora_dropout`     | 0.1          | Dropout rate applied to LoRA layers       |\n"
      ],
      "metadata": {
        "id": "04VpG-9gu57D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "training_config = {\n",
        "    'batch_size': 8,\n",
        "    'learning_rate': 3e-4,\n",
        "    'num_epochs': 2,  # Reduced for demonstration\n",
        "    'max_input_length': 512,\n",
        "    'max_target_length': 128,\n",
        "    'lora_r': 16,\n",
        "    'lora_alpha': 32,\n",
        "    'lora_dropout': 0.1\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for key, value in training_config.items():\n",
        "    print(f\"• {key}: {value}\")"
      ],
      "metadata": {
        "id": "5PqdwvCBoKz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `setup_lora_training`\n",
        "\n",
        "This function prepares the model for **LoRA (Low-Rank Adaptation)** fine-tuning by configuring which parts of the model will be updated and wrapping it accordingly.\n",
        "\n",
        "### Details:\n",
        "- **Target Modules**:  \n",
        "  The function selects specific submodules to apply LoRA based on the model architecture:  \n",
        "  - For **T5/FLAN-T5**: query (`q`), value (`v`), key (`k`), output (`o`), and feed-forward layers (`wi_0`, `wi_1`, `wo`).  \n",
        "  - For **BART-based models**: projection layers (`q_proj`, `v_proj`, `k_proj`, `out_proj`) and fully connected layers (`fc1`, `fc2`).\n",
        "\n",
        "- **LoRA Configuration**:  \n",
        "  Sets the rank (`r`), scaling factor (`alpha`), dropout, and task type for Seq2Seq language modeling.\n",
        "\n",
        "- **Model Wrapping**:  \n",
        "  Wraps the original model with LoRA using `get_peft_model`.\n",
        "\n",
        "- **Parameter Summary**:  \n",
        "  Prints the number and percentage of trainable parameters after applying LoRA, highlighting the efficiency gains.\n"
      ],
      "metadata": {
        "id": "pZ0TgieDvAyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_lora_training(model, model_type):\n",
        "    \"\"\"Setup LoRA configuration for the model\"\"\"\n",
        "\n",
        "    # Define target modules based on model architecture\n",
        "    if 'flan-t5' in model_type or 't5' in model_type:\n",
        "        target_modules = [\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\", \"wo\"]\n",
        "    else:  # BART-based models\n",
        "        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\", \"fc1\", \"fc2\"]\n",
        "\n",
        "    # LoRA configuration\n",
        "    lora_config = LoraConfig(\n",
        "        r=training_config['lora_r'],\n",
        "        lora_alpha=training_config['lora_alpha'],\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=training_config['lora_dropout'],\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to model\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Print trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "    total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "\n",
        "    print(f\"Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "    return peft_model"
      ],
      "metadata": {
        "id": "-w9OxWAToUir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Training the Model with LoRA\n",
        "\n",
        "This function handles the fine-tuning of the LoRA-adapted model using Hugging Face's `Trainer` API, incorporating efficient training strategies and evaluation.\n",
        "\n",
        "### Function: `train_model`\n",
        "\n",
        "### Key Components:\n",
        "- **Data Collator**: Prepares batches of tokenized inputs and labels with padding for uniform length.\n",
        "- **Training Arguments**: Configures training parameters such as batch size, learning rate, number of epochs, evaluation and saving frequency, and logging.\n",
        "- **Trainer Setup**: Creates a `Trainer` object for streamlined training and evaluation loops.\n",
        "- **Training Execution**: Measures and prints the total training time.\n",
        "\n"
      ],
      "metadata": {
        "id": "4t740o9tvO2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, tokenizer, train_dataset, val_dataset, model_name):\n",
        "    \"\"\"Train the model with LoRA\"\"\"\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results/{model_name}',\n",
        "        per_device_train_batch_size=training_config['batch_size'],\n",
        "        per_device_eval_batch_size=training_config['batch_size'],\n",
        "        learning_rate=training_config['learning_rate'],\n",
        "        num_train_epochs=training_config['num_epochs'],\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=1000,\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        report_to=None,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"Starting training...\")\n",
        "    start_time = time.time()\n",
        "    trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(f\"✅ Training completed in {training_time/60:.1f} minutes\")\n",
        "\n",
        "    return trainer, training_time"
      ],
      "metadata": {
        "id": "3d69uphnoVyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning Models and Evaluating Performance\n",
        "\n",
        "This loop performs the entire fine-tuning workflow for each selected model, including:\n",
        "\n",
        "1. **Loading** the pre-trained model and tokenizer.\n",
        "2. **Tokenizing** the training and validation datasets.\n",
        "3. **Applying LoRA** for parameter-efficient fine-tuning.\n",
        "4. **Training** the model using the `train_model()` function.\n",
        "5. **Generating summaries** on the test set with the fine-tuned model.\n",
        "6. **Evaluating** the generated summaries using ROUGE scores.\n",
        "7. **Displaying sample summaries** for qualitative inspection.\n",
        "8. **Saving results** (including the trained model and tokenizer) for further analysis.\n",
        "9. **Error handling** to continue training other models if one fails."
      ],
      "metadata": {
        "id": "rSe8UnuQvXsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_results = {}\n",
        "\n",
        "for model_display_name, model_info in models_info.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"🔄 TRAINING: {model_display_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model_name = model_info['model_name']\n",
        "    model_type = 'flan-t5' if 'flan-t5' in model_name else 't5' if 't5' in model_name else 'bart'\n",
        "\n",
        "    try:\n",
        "        # Load model and tokenizer\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Tokenize dataset\n",
        "        print(\"Tokenizing dataset...\")\n",
        "        train_tokenized = tokenize_data(\n",
        "            dataset['train'], tokenizer, model_type,\n",
        "            training_config['max_input_length'],\n",
        "            training_config['max_target_length']\n",
        "        )\n",
        "        val_tokenized = tokenize_data(\n",
        "            dataset['validation'], tokenizer, model_type,\n",
        "            training_config['max_input_length'],\n",
        "            training_config['max_target_length']\n",
        "        )\n",
        "\n",
        "        # Setup LoRA\n",
        "        print(\"Setting up LoRA...\")\n",
        "        peft_model = setup_lora_training(model, model_type)\n",
        "\n",
        "        # Train model\n",
        "        trainer, training_time = train_model(\n",
        "            peft_model, tokenizer, train_tokenized, val_tokenized, model_display_name\n",
        "        )\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"Evaluating on test set...\")\n",
        "        test_generated = generate_summaries(\n",
        "            peft_model, tokenizer, test_dialogues, model_type\n",
        "        )\n",
        "\n",
        "        rouge_scores = evaluate_summaries(test_generated, test_references)\n",
        "\n",
        "        # Store results\n",
        "        fine_tuned_results[model_display_name] = {\n",
        "            'rouge_scores': rouge_scores,\n",
        "            'training_time': training_time,\n",
        "            'generated_summaries': test_generated,\n",
        "            'model': peft_model,\n",
        "            'tokenizer': tokenizer\n",
        "        }\n",
        "\n",
        "        print(f\"Fine-tuned Results for {model_display_name}:\")\n",
        "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "        # Show sample results\n",
        "        show_sample_results(test_dialogues, test_references, test_generated,\n",
        "                           f\"{model_display_name} (Fine-tuned)\", num_samples=2)\n",
        "\n",
        "        # Clean up for next model (keep the trained model for later comparison)\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error training {model_display_name}: {e}\")\n",
        "        continue"
      ],
      "metadata": {
        "id": "CoQ48igyoaFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Comparing Zero-Shot vs Fine-Tuned Performance\n",
        "\n",
        "This step creates a consolidated view of each model’s summarization quality before and after fine-tuning.\n",
        "\n",
        "### Comparison Metrics:\n",
        "- **ROUGE-1, ROUGE-2, ROUGE-L** scores for both zero-shot and fine-tuned scenarios.\n",
        "- **Improvement** in ROUGE-1 to highlight gains from fine-tuning.\n"
      ],
      "metadata": {
        "id": "YfY3HgNZvjYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create comparison dataframe\n",
        "comparison_data = []\n",
        "\n",
        "for model_name in models_info.keys():\n",
        "    if model_name in zero_shot_results and model_name in fine_tuned_results:\n",
        "        zero_shot = zero_shot_results[model_name]['rouge_scores']\n",
        "        fine_tuned = fine_tuned_results[model_name]['rouge_scores']\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Model': model_name,\n",
        "            'Zero-shot ROUGE-1': zero_shot['rouge1'],\n",
        "            'Fine-tuned ROUGE-1': fine_tuned['rouge1'],\n",
        "            'Improvement': fine_tuned['rouge1'] - zero_shot['rouge1'],\n",
        "            'Zero-shot ROUGE-2': zero_shot['rouge2'],\n",
        "            'Fine-tuned ROUGE-2': fine_tuned['rouge2'],\n",
        "            'Zero-shot ROUGE-L': zero_shot['rougeL'],\n",
        "            'Fine-tuned ROUGE-L': fine_tuned['rougeL']\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"Performance Comparison (Zero-shot vs Fine-tuned):\")\n",
        "print(comparison_df.to_string(index=False, float_format='%.4f'))"
      ],
      "metadata": {
        "id": "82WOwHA0olSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
        "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    zero_shot_col = f'Zero-shot {metric}'\n",
        "    fine_tuned_col = f'Fine-tuned {metric}'\n",
        "\n",
        "    x = range(len(comparison_df))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[i].bar([p - width/2 for p in x], comparison_df[zero_shot_col],\n",
        "                width, label='Zero-shot', color=colors[i], alpha=0.7)\n",
        "    axes[i].bar([p + width/2 for p in x], comparison_df[fine_tuned_col],\n",
        "                width, label='Fine-tuned', color=colors[i])\n",
        "\n",
        "    axes[i].set_title(f'{metric} Comparison')\n",
        "    axes[i].set_xlabel('Models')\n",
        "    axes[i].set_ylabel('Score')\n",
        "    axes[i].set_xticks(x)\n",
        "    axes[i].set_xticklabels(comparison_df['Model'], rotation=45)\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0pKSZgecoqwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank models by fine-tuned performance\n",
        "final_ranking = comparison_df.sort_values('Fine-tuned ROUGE-1', ascending=False)\n",
        "\n",
        "print(\"🥇 FINAL RANKING (by ROUGE-1 score):\")\n",
        "for i, (_, row) in enumerate(final_ranking.iterrows(), 1):\n",
        "    print(f\"{i}. {row['Model']}: {row['Fine-tuned ROUGE-1']:.4f}\")\n",
        "\n",
        "best_model = final_ranking.iloc[0]\n",
        "print(f\"\\n🏆 WINNER: {best_model['Model']}\")\n",
        "print(f\"   Best ROUGE-1 Score: {best_model['Fine-tuned ROUGE-1']:.4f}\")\n",
        "print(f\"   Improvement from zero-shot: +{best_model['Improvement']:.4f}\")\n",
        "\n",
        "# Show detailed comparison of best model\n",
        "best_model_name = best_model['Model']\n",
        "if best_model_name in fine_tuned_results:\n",
        "    print(f\"\\n📝 DETAILED ANALYSIS OF BEST MODEL ({best_model_name}):\")\n",
        "    best_generated = fine_tuned_results[best_model_name]['generated_summaries']\n",
        "    show_sample_results(test_dialogues, test_references, best_generated,\n",
        "                       f\"{best_model_name} (WINNER)\", num_samples=5)"
      ],
      "metadata": {
        "id": "lNqD_GilouTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d5fe37a"
      },
      "source": [
        "### Save the fine-tuned model and tokenizer\n",
        "\n",
        "Now that you have fine-tuned the models, you can save the best-performing one (including its LoRA adapters) and its tokenizer to your local environment. You can then push these files to the Hugging Face Hub for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48fa3e5b"
      },
      "source": [
        "# Get the best model and tokenizer from the fine_tuned_results dictionary\n",
        "best_model_name = final_ranking.iloc[0]['Model']\n",
        "best_model_info = fine_tuned_results[best_model_name]\n",
        "best_peft_model = best_model_info['model']\n",
        "best_tokenizer = best_model_info['tokenizer']\n",
        "\n",
        "# Define output directory\n",
        "output_dir = f\"./{best_model_name}_fine_tuned\"\n",
        "\n",
        "# Save the PEFT model (LoRA adapters)\n",
        "best_peft_model.save_pretrained(output_dir)\n",
        "print(f\"Saved LoRA adapters to {output_dir}\")\n",
        "\n",
        "# Save the tokenizer\n",
        "best_tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Saved tokenizer to {output_dir}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa0c6dd2"
      },
      "source": [
        "After running the cell above, you will find a new directory named after your best performing model (e.g., `./FLAN-T5-Base_fine_tuned`). This directory contains the necessary files to push your fine-tuned model to the Hugging Face Hub.\n",
        "\n",
        "To push your model to the Hugging Face Hub:\n",
        "\n",
        "1.  Install the Hugging Face Hub library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dff5d52f"
      },
      "source": [
        "    from huggingface_hub import notebook_login\n",
        "    notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3272c28"
      },
      "source": [
        "    from huggingface_hub import HfApi\n",
        "    import os\n",
        "\n",
        "    api = HfApi()\n",
        "    repo_id = f\"sairika/{best_model_name}-dialogsum-lora\" # Replace 'your-username' with your Hugging Face username\n",
        "\n",
        "    # Create the repository on the Hub if it doesn't exist\n",
        "    api.create_repo(repo_id=repo_id, exist_ok=True)\n",
        "\n",
        "    # Upload all files from the output directory\n",
        "    api.upload_folder(\n",
        "        folder_path=output_dir,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully pushed model to https://huggingface.co/{repo_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3bf782a"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "from peft import AutoPeftModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Replace 'your-username' with your Hugging Face username and 'best-model-name' with the actual model name\n",
        "repo_id = f\"sairika/{best_model_name}-dialogsum-lora\"\n",
        "\n",
        "# Load the tokenizer and the PEFT model (which loads the base model and applies LoRA adapters)\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
        "    model = AutoPeftModelForSeq2SeqLM.from_pretrained(repo_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
        "    print(f\"✅ Model and tokenizer loaded successfully from {repo_id}\")\n",
        "\n",
        "    # Prepare test dialogues\n",
        "    test_dialogues_for_demo = [\n",
        "        \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, you never know. An annual check-up is important for early detection of any potential health issues.\\n#Person2#: I guess you're right. So, what do we do now?\\n#Person1#: I'll just do a quick examination and we'll discuss your medical history. We should also talk about quitting smoking.\\n#Person2#: I've been meaning to do that.\\n#Person1#: We have some programs and medications that can help. I'll give you some information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
        "        \"#Person1#: You're finally here! What took so long?\\n#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\\n#Person1#: It's always rather congested during peak hours. Have you considered taking public transport?\\n#Person2#: I have, but driving is just more convenient.\\n#Person1#: It might be convenient, but it's not good for your health or the environment. Plus, you wouldn't have to deal with traffic jams.\\n#Person2#: You have a point. I feel so bad about how much my car is adding to the pollution problem.\\n#Person1#: Exactly! And walking or biking to the bus or train station is great exercise. When the weather is nicer, you could even bike the whole way.\\n#Person2#: That's a good idea. I'll give public transport a try starting tomorrow.\\n#Person1#: Great! Let me know how it goes.\",\n",
        "        \"#Person1#: Excuse me, could you tell me how to get to the nearest subway station?\\n#Person2#: Sure. Go straight down this street for two blocks, then turn left. You'll see the station entrance on your right.\\n#Person1#: Straight down for two blocks, then left. Got it. Is it far?\\n#Person2#: Not really, about a five-minute walk.\\n#Person1#: Perfect. Thank you so much!\\n#Person2#: You're welcome. Have a good day.\"\n",
        "    ]\n",
        "\n",
        "    # Generate summaries\n",
        "    model.eval()\n",
        "    generated_summaries_for_demo = []\n",
        "\n",
        "    for dialogue in test_dialogues_for_demo:\n",
        "        model_type = 'flan-t5' if 'flan-t5' in repo_id else 't5' if 't5' in repo_id else 'bart'\n",
        "        prompt = create_prompts([dialogue], model_type)[0]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                num_beams=4,\n",
        "                length_penalty=0.6,\n",
        "                early_stopping=True,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_summaries_for_demo.append(summary)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n📝 Test Summaries:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, (dialogue, summary) in enumerate(zip(test_dialogues_for_demo, generated_summaries_for_demo)):\n",
        "        print(f\"Dialogue {i+1}: {dialogue[:200]}...\")\n",
        "        print(f\"Generated Summary {i+1}: {summary}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading or testing the model: {e}\")\n",
        "    print(f\"Please ensure the repository ID '{repo_id}' is correct and the model files were pushed successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a218e8fd"
      },
      "source": [
        "### What's Next?\n",
        "\n",
        "You have successfully fine-tuned a language model using LoRA, evaluated its performance against zero-shot baselines, and deployed the best-performing model to the Hugging Face Hub.\n",
        "\n",
        "Here's a summary of what you've achieved and what you can do next:\n",
        "\n",
        "1.  **Data Loading and Exploration**: Loaded and analyzed the DialogSum dataset.\n",
        "2.  **Model Selection**: Chose several models for summarization and compared their zero-shot performance.\n",
        "3.  **Fine-tuning with LoRA**: Applied LoRA to efficiently fine-tune the models on the DialogSum dataset.\n",
        "4.  **Evaluation**: Evaluated the fine-tuned models and compared their performance to the zero-shot results, demonstrating significant improvement.\n",
        "5.  **Deployment**: Saved the best-performing model's LoRA adapters and tokenizer and pushed them to the Hugging Face Hub.\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "*   **Integrate the Model**: You can now integrate the model hosted on the Hugging Face Hub into your own applications or workflows for dialogue summarization.\n",
        "*   **Further Fine-tuning**: Experiment with different LoRA configurations, training parameters, or larger datasets for potentially better performance.\n",
        "*   **Explore Other Architectures**: Try fine-tuning other transformer models suitable for summarization tasks.\n",
        "*   **Build a Demo**: Create a simple web demo using libraries like Gradio or Streamlit to showcase your fine-tuned model's capabilities."
      ]
    }
  ]
}